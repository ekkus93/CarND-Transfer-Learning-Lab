{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# y_train.shape is 2d, (50000, 1). While Keras is smart enough to handle this\n",
    "# it's a good idea to flatten the array.\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                      test_size=0.3, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from skimage import exposure\n",
    "from sklearn.utils import shuffle as shuffle_X_y\n",
    "\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def leaky_relu(x, alpha, name):\n",
    "    return tf.maximum(alpha * x, x, name) \n",
    "\n",
    "def conv_layer(x, num_input_channels, num_output_channels, \n",
    "               mu, sigma, window=(5,5)):\n",
    "    window_x,window_y = window\n",
    "    conv_W = tf.Variable(tf.truncated_normal(shape=(window_x, window_y, \n",
    "                                                    num_input_channels, num_output_channels), \n",
    "                                                     mean = mu, stddev = sigma))\n",
    "    conv_b = tf.Variable(tf.zeros(num_output_channels))\n",
    "    return tf.nn.conv2d(x, conv_W, strides=[1, 1, 1, 1], padding='SAME') + conv_b, conv_W\n",
    "\n",
    "def fully_connected(x, shape_in, shape_out, mu = 0, sigma = 0.1):\n",
    "    fc_W  = tf.Variable(tf.truncated_normal(shape=(shape_in, shape_out), \n",
    "                                            mean = mu, stddev = sigma))\n",
    "    fc_b  = tf.Variable(tf.zeros(shape_out))\n",
    "    return tf.matmul(x, fc_W) + fc_b, fc_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x, channel_depth, mu = 0, sigma = 1e-2, alpha=0.05, num_output=10): \n",
    "    ### Layer 1: Convolutional. Input = 32x32x10. Output = 32x32x16.\n",
    "    conv1, conv1_W = conv_layer(x, channel_depth, 16, mu, sigma)\n",
    "    print(conv1.get_shape())\n",
    "\n",
    "    # Leaky ReLU\n",
    "    conv1_activaton = leaky_relu(conv1, alpha, 'conv1_activaton')    \n",
    "\n",
    "    # Pooling. Input = 32x32x16. Output = 16x16x16.\n",
    "    conv1 = tf.nn.max_pool(conv1_activaton, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    print(conv1.get_shape())\n",
    "\n",
    "    ### Layer 2: Convolutional. Output = 16x16x32.\n",
    "    conv2, conv2_W = conv_layer(conv1, 16, 32, mu, sigma)\n",
    "    print(conv2.get_shape())\n",
    "\n",
    "    # Leaky ReLU\n",
    "    conv2_activaton = leaky_relu(conv2, alpha, 'conv2_activaton')  \n",
    "\n",
    "    # Pooling. Input = 16x16x32. Output = 8x8x32.\n",
    "    conv2 = tf.nn.max_pool(conv2_activaton, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    print(conv2.get_shape())\n",
    "\n",
    "    ### Layer 3: Convolutional. Output = 8x8x64.\n",
    "    conv3, conv3_W = conv_layer(conv2, 32, 64, mu, sigma)\n",
    "    print(conv3.get_shape())\n",
    "\n",
    "    # Leaky ReLU\n",
    "    conv3_activaton = leaky_relu(conv3, alpha, 'conv3_activaton')  \n",
    "\n",
    "    # Pooling. Input = 8x8x64. Output = 4x4x64.\n",
    "    conv3 = tf.nn.max_pool(conv3_activaton, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') \n",
    "    print(conv3.get_shape())\n",
    "\n",
    "    # Flatten. Input = 4x4x64. Output = 1024.\n",
    "    fc0   = flatten(conv3)\n",
    "\n",
    "    ### Layer 3: Fully Connected. Input = 1024. Output = 512.\n",
    "    fc1, fc1_W = fully_connected(fc0, 1024, 512, mu, sigma)\n",
    "\n",
    "    # Leaky ReLU\n",
    "    fc1 = leaky_relu(fc1, alpha, 'fc1_activation')  \n",
    "    \n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    ### Layer 4: Fully Connected. Input = 512. Output = 256.\n",
    "    fc2, fc2_W = fully_connected(fc1, 512, 256, mu, sigma)\n",
    "\n",
    "    # Leaky ReLU\n",
    "    fc2 = leaky_relu(fc2, alpha, 'fc2_activation')   \n",
    "    \n",
    "    # dropout\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)    \n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 256. Output = num_output.\n",
    "    logits, fc3_W = fully_connected(fc2, 256, num_output, mu, sigma)\n",
    "\n",
    "    return (logits, keep_prob, [conv1_W, conv2_W, conv3_W, fc1_W, fc2_W, fc3_W],\n",
    "            [conv1_activaton, conv2_activaton, conv3_activaton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_depth = 3\n",
    "num_outputs = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, channel_depth))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 16)\n",
      "(?, 16, 16, 16)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "rate = 1e-3       # learning rate\n",
    "alpha = 1e-05     # slope for negative input values for leaky ReLU's\n",
    "sigma = 1e-2      # std for initializing random weights\n",
    "beta = 1e-4       # multiplier for L2 regularization\n",
    "\n",
    "logits, keep_prob, weights, activations = model(x, channel_depth, sigma=sigma, alpha=alpha)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Loss function using L2 Regularization\n",
    "regularizer = None\n",
    "for weight in weights:\n",
    "    if regularizer is None:\n",
    "        regularizer = tf.nn.l2_loss(weight)\n",
    "    else:\n",
    "        regularizer = regularizer + tf.nn.l2_loss(weight)\n",
    "loss_operation = tf.reduce_mean(loss_operation + beta * regularizer)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits_argmax = tf.argmax(logits, 1, name='logits_argmax')\n",
    "one_hot_y_argmax = tf.argmax(one_hot_y, 1, name='one_hot_y_argmax')\n",
    "correct_prediction = tf.equal(logits_argmax, one_hot_y_argmax, name='correct_prediction')\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, batch_size, X_train_preprocessed, y_train, \n",
    "                X_valid_preprocessed, y_valid, model_name, keep_prob_val = 0.5,\n",
    "                max_to_keep=0):\n",
    "    saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "    model_dir = '%s/models/%s' % (data_dir, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    validation_accuracy_arr = []\n",
    "    \n",
    "    best_validation_accuracy_epoch = 0\n",
    "    best_validation_accuracy = 0.0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_examples = len(X_train_preprocessed)\n",
    "    \n",
    "        print(\"Training...\")\n",
    "        print()\n",
    "        for i in range(epochs):\n",
    "            X_train_preprocessed, y_train = shuffle_X_y(X_train_preprocessed, y_train)\n",
    "        \n",
    "            for offset in range(0, num_examples, batch_size):\n",
    "                end = offset + BATCH_SIZE\n",
    "                batch_x, batch_y = (X_train_preprocessed[offset:end], \n",
    "                                    y_train[offset:end])\n",
    "                sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, \n",
    "                                                        keep_prob: keep_prob_val})\n",
    "        \n",
    "            validation_accuracy = evaluate(X_valid_preprocessed, y_valid)\n",
    "            validation_accuracy_arr.append(validation_accuracy)\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        \n",
    "            if best_validation_accuracy < validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                best_validation_accuracy_epoch = i+1\n",
    "                saver.save(sess, '%s/%s' % (model_dir, model_name))\n",
    "                print(\"Model saved\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "    print(\"Best model - epoch: %d, best validation accuracy: %.3f\" % \n",
    "          (best_validation_accuracy_epoch, best_validation_accuracy))\n",
    "    \n",
    "    return validation_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_test_accuracy(X_test_preprocessed, y_test, model_name):\n",
    "    model_dir = '%s/models/%s' % (data_dir, model_name)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "\n",
    "        test_accuracy = evaluate(X_test_preprocessed, y_test)\n",
    "        print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_model(X, model_name, batch_size):\n",
    "    if X.shape[0] < batch_size:\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "    model_dir = '%s/models/%s' % (data_dir, model_name)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    logits_argmax_arr = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())  \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        \n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        while start_idx < X.shape[0]:\n",
    "            end_idx += batch_size\n",
    "            end_idx = min(end_idx, X.shape[0])\n",
    "            logits_argmax_arr.append(sess.run(logits_argmax, \n",
    "                                              feed_dict={x: X[start_idx:end_idx], \n",
    "                                                         keep_prob: 1.0}))\n",
    "            start_idx += batch_size\n",
    "        \n",
    "    result = np.concatenate(logits_argmax_arr, axis=0)\n",
    "    \n",
    "    assert result.shape[0] == X.shape[0]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 1500\n"
     ]
    }
   ],
   "source": [
    "train_cnt = int(0.1*X_train.shape[0])\n",
    "_X_train, _y_train = X_train[:train_cnt], y_train[:train_cnt]\n",
    "valid_cnt = int(0.1*X_valid.shape[0])\n",
    "_X_valid, _y_valid = X_valid[:valid_cnt], y_valid[:valid_cnt]\n",
    "print(train_cnt, valid_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###single_model\n",
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.126\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.171\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.186\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.203\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.270\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.309\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.309\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.331\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.347\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.305\n",
      "\n",
      "EPOCH 11 ...\n",
      "Validation Accuracy = 0.364\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 12 ...\n",
      "Validation Accuracy = 0.374\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 13 ...\n",
      "Validation Accuracy = 0.368\n",
      "\n",
      "EPOCH 14 ...\n",
      "Validation Accuracy = 0.386\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 15 ...\n",
      "Validation Accuracy = 0.373\n",
      "\n",
      "EPOCH 16 ...\n",
      "Validation Accuracy = 0.386\n",
      "\n",
      "EPOCH 17 ...\n",
      "Validation Accuracy = 0.382\n",
      "\n",
      "EPOCH 18 ...\n",
      "Validation Accuracy = 0.403\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 19 ...\n",
      "Validation Accuracy = 0.412\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 20 ...\n",
      "Validation Accuracy = 0.407\n",
      "\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.416\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 22 ...\n",
      "Validation Accuracy = 0.415\n",
      "\n",
      "EPOCH 23 ...\n",
      "Validation Accuracy = 0.416\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 24 ...\n",
      "Validation Accuracy = 0.421\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 25 ...\n",
      "Validation Accuracy = 0.419\n",
      "\n",
      "EPOCH 26 ...\n",
      "Validation Accuracy = 0.425\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 27 ...\n",
      "Validation Accuracy = 0.427\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 28 ...\n",
      "Validation Accuracy = 0.431\n",
      "INFO:tensorflow:./data/models/single_model/single_model is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved\n",
      "\n",
      "EPOCH 29 ...\n",
      "Validation Accuracy = 0.415\n",
      "\n",
      "EPOCH 30 ...\n",
      "Validation Accuracy = 0.407\n",
      "\n",
      "Best model - epoch: 28, best validation accuracy: 0.431\n",
      "Test Accuracy = 0.425\n"
     ]
    }
   ],
   "source": [
    "model_name = 'single_model' \n",
    "print(\"###%s\" % model_name)\n",
    "validation_accuracy_arr = train_model(EPOCHS, BATCH_SIZE, \n",
    "                                      _X_train, _y_train, \n",
    "                                      _X_valid, _y_valid, \n",
    "                                      model_name, keep_prob_val = 0.5, max_to_keep=0)    \n",
    "    \n",
    "display_test_accuracy(X_test, y_test, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carnd-term1",
   "language": "python",
   "name": "carnd-term1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
